{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf27669",
   "metadata": {},
   "source": [
    "# text generation using fnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99fb56f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\digit\\anaconda3\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\digit\\anaconda3\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\digit\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\digit\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\digit\\anaconda3\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\digit\\anaconda3\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\digit\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2025.1.31)\n",
      "Requirement already satisfied: six in c:\\users\\digit\\anaconda3\\lib\\site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\digit\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed7a5470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch[transformers] in c:\\users\\digit\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\digit\\anaconda3\\lib\\site-packages (from torch[transformers]) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from torch[transformers]) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from torch[transformers]) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\digit\\anaconda3\\lib\\site-packages (from torch[transformers]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from torch[transformers]) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\digit\\anaconda3\\lib\\site-packages (from torch[transformers]) (2023.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch[transformers]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\digit\\anaconda3\\lib\\site-packages (from jinja2->torch[transformers]) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: torch 2.7.0 does not provide the extra 'transformers'\n"
     ]
    }
   ],
   "source": [
    "!pip install torch[transformers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/wikitext-103-raw-v1 to C:/Users/digit/.cache/huggingface/datasets/parquet/wikitext-103-raw-v1-7bb180478b704b56/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b4c5f5fdf745218825f53eef80c2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2134a083d7594fb98352050963413185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7398eec52e4e16b7c8ec59d2b2a6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7aa914a1d744086970e6a199101bfa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea05cbf1a6d411d854bf87790ff9ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading dataset: [{'expected': SplitInfo(name='test', num_bytes=1305088, num_examples=4358, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='test', num_bytes=5176698, num_examples=17432, shard_lengths=None, dataset_name='parquet')}, {'expected': SplitInfo(name='train', num_bytes=546500949, num_examples=1801350, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='train', num_bytes=1113622699, num_examples=3676136, shard_lengths=[1650675, 1661350, 364111], dataset_name='parquet')}, {'expected': SplitInfo(name='validation', num_bytes=1159288, num_examples=3760, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='validation', num_bytes=4607450, num_examples=15040, shard_lengths=None, dataset_name='parquet')}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset splits explicitly\n",
    "try:\n",
    "    datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=['train', 'validation', 'test'])\n",
    "    # Assign splits to variables for clarity\n",
    "    train_dataset, validation_dataset, test_dataset = datasets\n",
    "    \n",
    "    # Print dataset info\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "    print(f\"Number of validation samples: {len(validation_dataset)}\")\n",
    "    print(f\"Number of test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Show an example\n",
    "    print(\"\\nSample from training data:\\n\", train_dataset[0])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error loading dataset:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentence\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Apply preprocessing\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(preprocess_text)\n\u001b[0;32m     12\u001b[0m datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(preprocess_text)\n\u001b[0;32m     13\u001b[0m datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(preprocess_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "    text = sentence[\"text\"].lower()\n",
    "    text = re.sub(\"[^a-z?!.,]\", \" \", text)\n",
    "    text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "    sentence[\"text\"] = text.strip()\n",
    "    return sentence\n",
    "\n",
    "# Apply preprocessing\n",
    "datasets[\"train\"] = datasets[\"train\"].map(preprocess_text)\n",
    "datasets[\"test\"] = datasets[\"test\"].map(preprocess_text)\n",
    "datasets[\"validation\"] = datasets[\"validation\"].map(preprocess_text)\n",
    "\n",
    "# Filter out short texts\n",
    "datasets[\"train\"] = datasets[\"train\"].filter(lambda x: len(x[\"text\"]) > 20)\n",
    "datasets[\"test\"] = datasets[\"test\"].filter(lambda x: len(x[\"text\"]) > 20)\n",
    "datasets[\"validation\"] = datasets[\"validation\"].filter(lambda x: len(x[\"text\"]) > 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f386167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\digit\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4261576b714240bb55d42bf4ff5916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\digit\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\digit\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e0085fd6154d75b141ff92fb536593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164cd52d64784f16b8a59774ee2e1bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m tokenizer(sentence[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentence\n\u001b[1;32m---> 13\u001b[0m tokenized_inputs \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(tokenize)\n\u001b[0;32m     14\u001b[0m tokenized_inputs \u001b[38;5;241m=\u001b[39m tokenized_inputs\u001b[38;5;241m.\u001b[39mremove_columns([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# DataCollator\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding \n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Tokenizer\n",
    "def tokenize(sentence):\n",
    "    sentence = tokenizer(sentence['text'], truncation=True)\n",
    "    return sentence\n",
    "\n",
    "tokenized_inputs = datasets['test'].map(tokenize)\n",
    "tokenized_inputs = tokenized_inputs.remove_columns(['text'])\n",
    "# DataCollator\n",
    "batch = 16\n",
    "data_collator = DataCollatorWithPadding(\n",
    " \n",
    "   tokenizer=tokenizer, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "dataloader = DataLoader(tokenized_inputs, batch_size=batch, collate_fn=data_collator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce3bbd6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression (1998605791.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 27\u001b[1;36m\u001b[0m\n\u001b[1;33m    positional_encoding=[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / self.d_model)))\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot assign to expression\n"
     ]
    }
   ],
   "source": [
    "# Step 5 : Adding the Positioning embedding\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft as fft\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.positional_encoding = self.create_positional_encoding().to(device)\n",
    "\n",
    "    def create_positional_encoding(self):\n",
    "        # Initialize positional encoding matrix\n",
    "        positional_encoding = np.zeros((self.max_sequence_length, self.d_model))\n",
    "\n",
    "        for pos in range(self.max_sequence_length):\n",
    "           for i in range(0, self.d_model, 2):\n",
    "        # For even indices apply sin\n",
    "            positional_encoding=[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "        # For odd indices apply cos\n",
    "            if i + 1 < self.d_model:\n",
    "               positional_encoding=[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "\n",
    "# Return positional encoding tensor\n",
    "        return torch.from_numpy(positional_encoding).float()\n",
    "\n",
    "def forward(self, x):\n",
    "    # Add position embedding which is encoded in self.positional_encoding\n",
    "    expanded_tensor = torch.unsqueeze(self.positional_encoding, 0).expand(x.size(0), -1, -1).to(device)\n",
    "\n",
    "    return x.to(device) + expanded_tensor[:,:x.size(1), :]\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "  def __init__(self, sequence_length, vocab_size, embed_dim):\n",
    "    super(PositionalEmbedding, self).__init__()\n",
    "    self.token_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "    self.position_embeddings = PositionalEncoding(embed_dim,sequence_length)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    embedded_tokens = self.token_embeddings(inputs).to(device)\n",
    "    embedded_positions = self.position_embeddings(embedded_tokens).to(device)\n",
    "    return embedded_positions.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1e57b4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 6 : Create FNet Encoder\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFNetEncoder\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,embed_dim, dense_dim):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28msuper\u001b[39m(FNetEncoder,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 6 : Create FNet Encoder\n",
    "class FNetEncoder(nn.Module):\n",
    "\n",
    "  def __init__(self,embed_dim, dense_dim):\n",
    "    super(FNetEncoder,self).__init__()\n",
    "    self.embed_dim = embed_dim\n",
    "    self.dense_dim = dense_dim\n",
    "    self.dense_proj = nn.Sequential(nn.Linear(self.embed_dim,self.dense_dim), nn.ReLU(), nn.Linear(self.dense_dim,self.embed_dim))\n",
    "\n",
    "    self.layernorm_1 = nn.LayerNorm(self.embed_dim)\n",
    "    self.layernorm_2 = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "  def forward(self,inputs):\n",
    "\n",
    "    fft_result = fft.fft2(inputs)\n",
    "\n",
    "    #taking real part\n",
    "    fft_real = fft_result.real.float()\n",
    "\n",
    "    proj_input = self.layernorm_1 (inputs + fft_real)\n",
    "    proj_output = self.dense_proj(proj_input)\n",
    "    return self.layernorm_2(proj_input +proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e02c58c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 7 : Create FnetDecoder\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFNetDecoder\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,embed_dim,dense_dim,num_heads):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28msuper\u001b[39m(FNetDecoder,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 7 : Create FnetDecoder\n",
    "class FNetDecoder(nn.Module):\n",
    "\n",
    "  def __init__(self,embed_dim,dense_dim,num_heads):\n",
    "    super(FNetDecoder,self).__init__()\n",
    "    self.embed_dim = embed_dim\n",
    "    self.dense_dim = dense_dim\n",
    "    self.num_heads = num_heads\n",
    "\n",
    "    self.attention_1 = nn.MultiheadAttention(embed_dim,num_heads,batch_first=True)\n",
    "    self.attention_2 = nn.MultiheadAttention(embed_dim,num_heads,batch_first=True)\n",
    "\n",
    "    self.dense_proj = nn.Sequential(nn.Linear(embed_dim, dense_dim),nn.ReLU(),nn.Linear(dense_dim, embed_dim))\n",
    "\n",
    "    self.layernorm_1 = nn.LayerNorm(embed_dim)\n",
    "    self.layernorm_2 = nn.LayerNorm(embed_dim)\n",
    "    self.layernorm_3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "  def forward(self, inputs, encoder_outputs, mask=None):\n",
    "    causal_mask = nn.Transformer.generate_square_subsequent_mask(inputs.size(1)).to(device)\n",
    "\n",
    "    attention_output_1, _ = self.attention_1(inputs, inputs, inputs, attn_mask=causal_mask)\n",
    "    out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "    if mask!= None:\n",
    "      attention_output_2, _ = self.attention_2(out_1, encoder_outputs, encoder_outputs, key_padding_mask =torch.transpose(mask, 0, 1).to(device))\n",
    "    else:\n",
    "     attention_output_2, _ = self.attention_2(out_1, encoder_outputs, encoder_outputs)\n",
    "    out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "    proj_output = self.dense_proj(out_2)\n",
    "    return self.layernorm_3(out_2 + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4866c9b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 8 : Fnet Model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFNetModel\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_length, vocab_size, embed_dim, latent_dim, num_heads):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(FNetModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 8 : Fnet Model\n",
    "class FNetModel(nn.Module):\n",
    "    def __init__(self, max_length, vocab_size, embed_dim, latent_dim, num_heads):\n",
    "        super(FNetModel, self).__init__()\n",
    "\n",
    "        self.encoder_inputs = PositionalEmbedding(max_length,vocab_size, embed_dim)\n",
    "        self.encoder1 = FNetEncoder(embed_dim, latent_dim)\n",
    "        self.encoder2 = FNetEncoder(embed_dim, latent_dim)\n",
    "        self.encoder3 = FNetEncoder(embed_dim, latent_dim)\n",
    "        self.encoder4 = FNetEncoder(embed_dim, latent_dim)\n",
    "\n",
    "        self.decoder_inputs = PositionalEmbedding(max_length,vocab_size, embed_dim)\n",
    "        self.decoder1 = FNetDecoder(embed_dim, latent_dim, num_heads)\n",
    "        self.decoder2 = FNetDecoder(embed_dim, latent_dim, num_heads)\n",
    "        self.decoder3 = FNetDecoder(embed_dim, latent_dim, num_heads)\n",
    "        self.decoder4 = FNetDecoder(embed_dim, latent_dim, num_heads)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def encoder(self,encoder_inputs):\n",
    "        x_encoder = self.encoder_inputs(encoder_inputs)\n",
    "        x_encoder = self.encoder1(x_encoder)\n",
    "        x_encoder = self.encoder2(x_encoder)\n",
    "        x_encoder = self.encoder3(x_encoder)\n",
    "        x_encoder = self.encoder4(x_encoder)\n",
    "        return x_encoder\n",
    "\n",
    "    def decoder(self,decoder_inputs,encoder_output,att_mask):\n",
    "        x_decoder = self.decoder_inputs(decoder_inputs)\n",
    "        x_decoder = self.decoder1(x_decoder, encoder_output,att_mask) ## HERE for inference\n",
    "        x_decoder = self.decoder2(x_decoder, encoder_output,att_mask) ## HERE for inference\n",
    "        x_decoder = self.decoder3(x_decoder, encoder_output,att_mask) ## HERE for inference\n",
    "        x_decoder = self.decoder4(x_decoder, encoder_output,att_mask) ## HERE for inference\n",
    "        decoder_outputs = self.dense(x_decoder)\n",
    "\n",
    "        return decoder_outputs\n",
    "\n",
    "    def forward(self, encoder_inputs, decoder_inputs,att_mask = None):\n",
    "        encoder_output = self.encoder(encoder_inputs)\n",
    "        decoder_output = self.decoder(decoder_inputs,encoder_output,att_mask=None)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0034b023",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FNetModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m NUM_HEADS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m fnet_model \u001b[38;5;241m=\u001b[39m FNetModel(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM, LATENT_DIM, NUM_HEADS)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FNetModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 9 : Initialize Model\n",
    "# Assuming your constants are defined as follows:\n",
    "MAX_LENGTH = 512\n",
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "EMBED_DIM = 256\n",
    "LATENT_DIM = 100\n",
    "NUM_HEADS = 4\n",
    "\n",
    "# Instantiate the model\n",
    "fnet_model = FNetModel(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM, LATENT_DIM, NUM_HEADS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba792b79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fnet_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 10 : Train the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Define your optimizer and loss function\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(fnet_model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m      4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fnet_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 10 : Train the model\n",
    "# Define your optimizer and loss function\n",
    "optimizer = torch.optim.Adam(fnet_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for batch in dataloader:\n",
    "        encoder_inputs_tensor = batch['input_ids'][:,:-1].to(device)\n",
    "        decoder_inputs_tensor = batch['input_ids'][:,1:].to(device)\n",
    "\n",
    "        att_mask = batch['attention_mask'][:,:-1].to(device).to(dtype=bool)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = fnet_model(encoder_inputs_tensor, decoder_inputs_tensor,att_mask)\n",
    "        decoder_inputs_tensor.masked_fill(batch['attention_mask'][:,1:].ne(1).to(device), -100).to(device)\n",
    "\n",
    "        loss = criterion(outputs.view(-1, VOCAB_SIZE), decoder_inputs_tensor.reshape(-1))\n",
    "        train_loss = train_loss + loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "print (f\"epoch: {epoch}, train_loss : {train_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
